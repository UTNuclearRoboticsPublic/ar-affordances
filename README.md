# Defining Robot Manipulations with Augmented Reality Demonstrations

## About
This is a novel end-to-end system that captures a single manipulation task demonstration from an augmented reality (AR) head-mounted display (HMD), computes an affordance primitive (AP) representation of the task, and sends the task parameters to a mobile manipulator for execution in real-time. The system is robust, generalizable, and mobile. The system is intended for those who work along side mobile manipulators in unknown and unstructured environments. To learn more, watch the demonstration video hyperlinked below or read the 2023 IEEE IROS conference paper cited below. 

## Demonstration Video
[![YouTube Demonstration Video](https://user-images.githubusercontent.com/84527482/222321638-8ced7798-70ca-40a6-8df2-5a5c11380408.png)](https://www.youtube.com/watch?v=5AKIhkXAiO4&ab_channel=Nuclear%26AppliedRoboticsGroup)
Click on the video above to watch!

## Citation
```
@inproceedings{regalUsingSingleDemonstrations2023,
  title = {Using Single Demonstrations to Define Autonomous Manipulation Contact Tasks in Unstructured Environments via Object Affordances},
  author = {Regal, Frank and Pettinger, Adam and Duncan, John and Parra, Fabian and Akita, Emmanuel and Navarro, Alex and Pryor, Mitch},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={},
  year = {2023},
  organization={IEEE}
}
```
